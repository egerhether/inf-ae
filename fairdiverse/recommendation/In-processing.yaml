{
   ############base model#########################
   model: BPR,
   data_type: 'pair', #[point, pair, sequential]
   #############################################################

   ##Should the preprocessing be redone based on the new parameters instead of using the cached files in ~/recommendation/process_dataset######
   reprocess: False,
   ###############################################

   ####fair-rank model settings
   fair-rank: True, ##if you want to run a fair-rank module on the base models, you should set the value as True
   rank_model: 'APR',
   #################


   # LLM recommendation setting
   use_llm: False,
   llm_type: vllm,     # choose from ['api', 'local', 'vllm']
   llm_name: Mistral-7B,     # choose from ['Llama3-8B', 'Qwen2-7B', 'Mistral-7B']
   grounding_model: Qwen2-7B,   #  choose from ['Llama3-8B', 'Qwen2-7B' 'bert', 'gpt2']
   saved_embs_filename: "qwen_embs.pt",  # choose from [None, 'path']
   fair_prompt: "You are a item-fair recommender. Please try to ensure that each category of items receives fair recommendations.",
   #############################################

  #############log name, it will store the evaluation result in ~log/your_log_name/
   log_name: "test",
  #################################################

   ###########################training parameters################################
   device: cpu,
   epoch: 20,
   batch_size: 64,
   learning_rate: 0.001,
   ###########################################################################


   ###################################evaluation parameters: overwrite from ~/properties/evaluation.yaml######################################
   mmf_eval_ratio: 0.5,
   decimals: 4,
   eval_step: 5,
   eval_type: 'ranking',
   watch_metric: 'mmf@20',
   topk: [ 5,10,20 ], # if you choose the ranking settings, you can choose your top-k list
   store_scores: True, #If set true, the all relevance scores will be stored in the ~/log/your_name/ for post-processing
   fairness_type: "Exposure", # ["Exposure", "Utility"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups
   ###########################################################################
}